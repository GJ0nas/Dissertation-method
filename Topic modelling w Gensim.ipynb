{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wordninja\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import wordninja\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel(r'xxxx', index= 0, encoding= 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks whether this is consistent with number of articles for each year\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hlead']= df['Hlead'].str.lower()\n",
    "df['Hlead']= df['Hlead'].str.replace(r'\\d+', '')\n",
    "df['Hlead']= df['Hlead'].str.replace(r'block-time', '')\n",
    "df['Hlead']= df['Hlead'].str.replace(r'published-time', '')\n",
    "df['Hlead']= df['Hlead'].str.replace(r'gmt', '')\n",
    "df['Hlead']= df['Hlead'].str.replace(r'bst', '')\n",
    "df['Hlead']= df['Hlead'].str.replace(r'aest', '')\n",
    "df['Hlead']=df['Hlead'].str.replace(r'summary', '')\n",
    "df['Hlead']= df['Hlead'].str.replace(r'pm', '')\n",
    "\n",
    "for sentences in df['Hlead']:\n",
    "    wordninja.split(sentences)\n",
    "    \n",
    "def remove_punctuation(text):\n",
    "    text= ''.join([i for i in text if i not in string.punctuation])\n",
    "    return text\n",
    "df['Hlead'] = df['Hlead'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['Hlead']= df['Hlead'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(text):\n",
    "    text= [w for w in text if w not in stopwords.words('english')]\n",
    "    return text\n",
    "\n",
    "df['Hlead'] = df['Hlead'].apply(lambda x: remove_stops(x))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text= [lemmatizer.lemmatize(i) for i in text]\n",
    "    return text\n",
    "\n",
    "df['Hlead']= df['Hlead'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'xxxx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel(r'xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1)\n",
    "df[\"Hlead\"] = df[\"Hlead\"].map(lambda x: x.split(' ')) # didn't need this once I had switched around stopwords and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(df['Hlead']) #Creates a dictionary which maps strings to integers. This is same as doc-term matrix\n",
    "corpus= [dictionary.doc2bow(text) for text in df['Hlead']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus= vectorised corpus\n",
    "#num_topics= self-explanatory\n",
    "#iterations= the number of passes your model makes over the dataset\n",
    "#alpha= document-topic density. The higher the alpha, the more topics within a doc, and lower= vice versa\n",
    "#id2word= df['hlead'] converted into a gensim dictionary which maps strings to integers. \n",
    "#random_state= either a randomState object o a seed to generate one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random state is consistent\n",
    "#Period 1, K= 22, iter= 1500, coherence= 0.39 \n",
    "#Period 2, K= 20 , iter= 1500, coherence= 0.46\n",
    "#Period 3, K= 28??, iter= 1500, coherence= 0.44\n",
    "\n",
    "k= 20 #num of topics\n",
    "lda= LdaModel(corpus, num_topics=k, iterations= 1500, alpha= 'auto', id2word= dictionary, random_state=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents on the rows and topic proportions on the columns\n",
    "theta = lda.get_document_topics(corpus, minimum_probability = 0)\n",
    "\n",
    "# Topics on the rows and words on the columns\n",
    "phi = lda.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score using c_v\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=df['Hlead'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start= value to start from, limit= where to go up to, step= increments in\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=df['Hlead'], start=15, limit=75, step=5)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=75; start=15; step=5;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic interpretation (analysis)\n",
    "\n",
    "\n",
    "- Start by analysing the keywords associated with each topic\n",
    "- Proceed to examine the assignment frequencies (i.e. examine how much each keyword occurs in a topic)\n",
    "- Analysing the semantic coherence (i.e. how much the topics make sense?)\n",
    "\n",
    "## Keywords\n",
    "\n",
    "We start the process of topic interpretation and validation by examining the topic **keywords**. These \"keywords\" are probable tokens under the model -- i.e., the tokens most often assigned to a particular topic. We can either analyse each topic individually using the `show_topic()` or all at the same time using `show_topics()` method for LDA objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = lda.show_topics(num_topics=k, num_words =20 )\n",
    "for key in keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nice visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def view_keywords(model, num_topics, num_words = 20, prettyprint = True):\n",
    "# Return keywords from gensim\n",
    "    keywords = model.show_topics(num_topics = num_topics, \n",
    "                               num_words = num_words, \n",
    "                               formatted=False)\n",
    "\n",
    "    # Reformat keyword results for easy viewing\n",
    "    output = []\n",
    "    for row in keywords:\n",
    "        tokens = ' '.join([token[0] for token in row[1]])\n",
    "        output.append([row[0], tokens])\n",
    "    \n",
    "    # Print a nicely formatted table\n",
    "    if prettyprint:\n",
    "        tbl = PrettyTable()\n",
    "                \n",
    "        # Column labels\n",
    "        tbl.field_names = [\"Topic ID\", \"Keywords\"]\n",
    "        \n",
    "        # Populate table\n",
    "        for row in output:\n",
    "            tbl.add_row(row)\n",
    "        \n",
    "        # Output formatted table\n",
    "        tbl.align = \"l\"\n",
    "        print(tbl)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#outlined box does have words ranked in order of prevalence\n",
    "keywords = view_keywords(lda, lda.num_topics, num_words = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining what a topic is about\n",
    "This section extracts the most probable documents for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_documents(content, topic_id, theta, n = 10):\n",
    "  \n",
    "    x = theta[topic_id,:].todense()\n",
    "    idx = np.argpartition(x[0,:], -n)[0,-n:]\n",
    "    \n",
    "    # Get sorted IDs\n",
    "    idx_sorted = idx[0, np.argsort(-x[0,idx])].tolist()[0]\n",
    "    \n",
    "    # Find and return the top documents\n",
    "    return [row for i,row in enumerate(content) if i in set(idx_sorted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Extracting the full theta matrix. This can be slow!')\n",
    "theta = gensim.matutils.corpus2csc(lda[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs = top_documents(df['Hlead'],0, theta) #this calls your function. The number refers to the topic \n",
    "print(top_docs[2]) #should print off the top document for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic assignment frequency\n",
    "\n",
    "Next, I like to extract on overall measure of the **importance** of a topic to a corpus. Here, we can define a function to extract the topic assignment frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the topic frequency\n",
    "def topic_frequency(model, corpus, proportion = True, LOG_EVERY_N = 1000):\n",
    "    ''' Takes a gensim model object and a corpus object\n",
    "        and returns the number of words assigned to each\n",
    "        topic. '''\n",
    "    \n",
    "    # Extract topic distributions\n",
    "    theta = model.get_document_topics(corpus, minimum_probability = 0)\n",
    "    \n",
    "    # Extract number of words in each document\n",
    "    n = [sum([row[1] for row in doc]) for doc in corpus]\n",
    "    \n",
    "    # Get topic assignments\n",
    "    print('Extracting topic assignments for each token...')\n",
    "    \n",
    "    counts = []\n",
    "    for i,row in enumerate(theta):\n",
    "        # Extract topic assignemnt counts\n",
    "        counts.append([round(el[1]*n[i]) for el in row])\n",
    "        \n",
    "        # Log progress\n",
    "        if (i % LOG_EVERY_N) == 0:\n",
    "            print('Finished processing %s documents' % i)\n",
    "            \n",
    "    # Convert to a numpy array\n",
    "    counts_matrix = np.array(counts)\n",
    "    \n",
    "    # Sum down topics to get assignment totals\n",
    "    assignments = np.sum(counts_matrix, axis = 0)\n",
    "    \n",
    "    if proportion:\n",
    "        res = assignments/np.sum(assignments)\n",
    "    else:\n",
    "        res = assignments\n",
    "    \n",
    "    return res.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the assignment proportions by calling the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topic assignments\n",
    "assignments = topic_frequency(lda, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below loop prints out the assignment proportions to get a sense of topic prevalence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(assignments):\n",
    "    print('Topic %s = %s' % (i, round(row, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic \"quality\"\n",
    "\n",
    "### Semantic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out and tokenize the topic keywords\n",
    "topics = [row[1].split(' ') for row in keywords]\n",
    "\n",
    "# Estimate coherence. The 'u_mass' and 'c_v'\n",
    "# methods are good to try.\n",
    "co = CoherenceModel(topics=topics,\n",
    "                    texts=df['Hlead'],\n",
    "                    dictionary=dictionary,\n",
    "                    coherence='c_v')\n",
    "\n",
    "# Extract semantic coherence for each topic\n",
    "semantic = co.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(semantic):\n",
    "    print('Topic %s = %s' % (i, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model coherence = %s' % np.mean(semantic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_to_write = []\n",
    "for i,topic in enumerate(topics):\n",
    "    topics_to_write.append([i, assignments[i], semantic[i], ex[i], ' '.join(topic)])\n",
    "\n",
    "# Insert labels\n",
    "topics_to_write.insert(0, ['id', 'assignment_prop', 'coherence', 'exclusivity', 'keywords'])\n",
    "\n",
    "# Write\n",
    "with open('xxxx', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(topics_to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
