{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import wordninja\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\garet\\Documents\\Python Scripts\\OneDrive\\Dissertation\\Method\\Coding\\Validation sheet for hand coding.xlsx', encoding= 'UTF-8')\n",
    "print('File loaded as Excel Workbook')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(['Title', 'Publication', 'Headline', 'Short title', 'Short article', 'st', 'sa', 'Unnamed: 5', 'Unnamed: 6'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hlead'].isnull().sum() #identifies any nan values (non present in df)\n",
    "df['Hlead']= df['Hlead'].astype(str) #ignore nan values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hlead']= df['Hlead'].str.lower()\n",
    "df['Hlead']= df['Hlead'].str.replace('\\d+', '')\n",
    "df['Hlead']= df['Hlead'].str.replace('block-time', '')\n",
    "df['Hlead']= df['Hlead'].str.replace('published-time', '')\n",
    "df['Hlead']= df['Hlead'].str.replace('gmt', '')\n",
    "df['Hlead']= df['Hlead'].str.replace('bst', '')\n",
    "df['Hlead']= df['Hlead'].str.replace('aest', '')\n",
    "df['Hlead']=df['Hlead'].str.replace('summary', '')\n",
    "df['Hlead']= df['Hlead'].str.replace('pm', '')\n",
    "\n",
    "for sentences in df['Hlead']:\n",
    "    wordninja.split(sentences)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text= ''.join([i for i in text if i not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "df['Hlead'] = df['Hlead'].apply(remove_punctuation)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['Hlead']= df['Hlead'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword removal and stemming\n",
    "def remove_stops(text):\n",
    "    text= [w for w in text if w not in stopwords.words('english')]\n",
    "    return text\n",
    "\n",
    "df['Hlead'] = df['Hlead'].apply(lambda x: remove_stops(x))\n",
    "print('done')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text= ' '.join([stemmer.stem(i) for i in text])\n",
    "    return stem_text\n",
    "\n",
    "df['Hlead'] = df['Hlead'].apply(lambda x: word_stemmer(x))\n",
    "print(df['Hlead'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency count with SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel(r'C:\\Users\\garet\\Documents\\Python Scripts\\OneDrive\\Dissertation\\Method\\Coding\\Dictionary\\Dictionary count validation', index=False)\n",
    "#print('''Saving complete. Year is now processed into two files (both in excel format): \n",
    "      #(1) preprocessed and (2) pre_processed and stemmed''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer() #Instantiates SKlearn module\n",
    "dtm = vectorizer.fit_transform(df['Hlead']).todense() #vectorises texts & transforms into documnet-term-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape #shows that there are x rows with x unique words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = vectorizer.get_feature_names()\n",
    "index_dict = vectorizer.vocabulary_\n",
    "\n",
    "keywords = ['abat', 'abm', 'abmt', 'abughraib', 'afghanistan', 'aggress', 'aid', 'airland', 'airpow', 'alert', 'allianc', 'alqaeda', 'alshabaab', 'ammunit', 'amphibi', 'anarchi', 'anthrax', 'antipersonnel', 'apt', 'arab', 'arm', 'armi', 'armedforc', 'armscontrol', 'armsrac', 'armstrad', 'assad', 'assassin', 'assault', 'asylum', 'asylumseek', 'atom', 'attack', 'author', 'baader', 'balanc', 'ballist', 'ballisticmissil', 'barrag', 'battl', 'binladen', 'bipolar', 'blitzkrieg', 'block', 'bloc', 'bokoharam', 'bomb', 'bombard', 'breivik', 'bullet', 'capabl', 'capitul', 'casualti', 'catastroph', 'ceasefir', 'central', 'intellig', 'agenc', 'charliehebdo', 'chechnya', 'cia', 'cluster', 'coerc', 'coercion', 'coerciv', 'collaps', 'coloni', 'combat', 'comp', 'comb', 'compel', 'compromis', 'concess', 'concili', 'conflict', 'congo', 'contain', 'control', 'cooper', 'counteract', 'counterinsurg', 'counterterror', 'counterterrorist', 'countervail', 'coup', 'crime', 'crimin', 'crisi', 'critic', 'cyberspher', 'cyberwar', 'daesh', 'damag', 'danger', 'decis', 'defenc', 'defend', 'defens', 'demilitaris', 'demilitiaris', 'demilitar', 'demobilis', 'destabilis', 'destabil', 'destruct', 'd√©tent', 'destructionist', 'deter', 'deterr', 'dictat', 'dilemma', 'disarm', 'disarma', 'disast', 'diseas', 'disintegr', 'disobedi', 'disput', 'dissent', 'dissid', 'divis', 'domin', 'embargo', 'emerg', 'enemi', 'escal', 'evil', 'existenti', 'expeditionari', 'exploit', 'explos', 'extraordinari', 'faction', 'failedst', 'fascism', 'fascist', 'fear', 'fight', 'fought', 'firearm', 'fighter', 'firefight', 'fln', 'forbid', 'forbad', 'forbidden', 'forc', 'freedom', 'friction', 'fundament', 'fundamentalist', 'gang', 'ga', 'gass', 'gaz', 'genocid', 'guerrilla', 'guevara', 'gulf', 'hack', 'hacker', 'hama', 'hard', 'hazard', 'hebdo', 'hegemon', 'hegemoni', 'hezbollah', 'hijack', 'hiroshima', 'hiv', 'homeland', 'hostag', 'hussein', 'iaea', 'icc', 'ident', 'illeg', 'illegalis', 'illicit', 'immin', 'imminin', 'imperi', 'imperialist', 'incumb', 'infiltr', 'inhuman', 'insecur', 'insurg', 'interdepend', 'interdict', 'interpol', 'intervent', 'invad', 'invas', 'ira', 'iran', 'iraq', 'isil', 'isi', 'israel', 'isra', 'jihadi', 'jihad', 'jihadist', 'kidnap', 'kill', 'korea', 'kosovo', 'kurd', 'kurdistan', 'kuwait', 'landmin', 'law', 'lebanon', 'liber', 'libya', 'malacca', 'mercaneri', 'migrant', 'migrat', 'militarili', 'militar', 'militari', 'militarist', 'militaryindustrialcomplex', 'militia', 'militiaman', 'militiamen', 'misogyni', 'missil', 'munit', 'munition', 'nagasaki', 'narcot', 'nation', 'nationalist', 'nato', 'nazi', 'nazism', 'netwar', 'nonprolifer', 'nuclear', 'offens', 'oil', 'osc', 'overwhelm', 'pakistan', 'palestin', 'paramilitari', 'partisan', 'phillipin', 'plo', 'polic', 'poverti', 'power', 'preempt', 'preemption', 'prevent', 'prolifer', 'protect', 'punit', 'racism', 'racist', 'radic', 'radicalis', 'rape', 'refuge', 'rescu', 'resist', 'resolut', 'resourc', 'respons', 'retali', 'reveng', 'revolut', 'ricin', 'rival', 'rogu', 'rwanda', 'sadam', 'safe', 'salt', 'salw', 'sanction', 'sarin', 'saudism', 'scarc', 'scarcer', 'scarcest', 'scarciti', 'seapow', 'secur', 'securitis', 'securit', 'separatist', 'separat', 'septemb', '11', 'shock', 'shortag', 'somalia', 'sovereign', 'sovereignti', 'stabilis', 'strategi', 'strateg', 'strike', 'suicid', 'superpow', 'surg', 'surveil', 'syria', 'tactic', 'tactician', 'target', 'terror', 'terrorist', 'threat', 'threaten', 'tortur', 'traffic', 'traffick', 'transnat', 'uae', 'upris', 'valu', 'vietnam', 'violenc', 'violent', 'war', 'warcrim', 'warcrimin', 'warlord', 'weapon', 'wmd', 'wound'] #your list of keywords connoting securitization will go in here. \n",
    "\n",
    "key_word_index = [] \n",
    "for idx in keywords: #this function looks for all instances of my keywords in the dictionary and returns words which are not mentioned. \n",
    "    try:\n",
    "        key_word_index.append(index_dict[idx])\n",
    "    except:\n",
    "        print(\"Can't find %s\" % idx) #this will be useful to show which words do not feature at all in the design. \n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "securitizing_words =[] \n",
    "#this looks at each unique instance in the dtm and appends the empty securitizing_words list to include each word in the dtm..\n",
    "#which is also in the keyword index (so that no words that are not in the index are in there) and adds it together.\n",
    "for i in range(dtm.shape[0]):\n",
    "    securitizing_words.append(dtm[i,key_word_index].sum())\n",
    "\n",
    "counts = pd.Series(securitizing_words).value_counts() #converts securitizing words into a pandas df with the counts of the unique values.\n",
    "#The DF is in descending order= first element is the most frequently_occurring.\n",
    "print(counts) #return frequency count of amount of times word occurs per article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.to_excel(r'C:\\Users\\garet\\Documents\\Python Scripts\\OneDrive\\Dissertation\\Method\\Coding\\Dictionary\\Dictionary count for validation.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
